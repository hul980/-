{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"감성분석 - (한글)네이버 영화 리뷰와 포도사전을 토대로 웹툰댓글 감성분석.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"code","metadata":{"id":"891a2496"},"source":["#https://wikidocs.net/44249 에서 네이버 영화리뷰를 기반으로 한 감성분석방법을 가져왔습니다."],"id":"891a2496","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHnOMBRDSNpy","executionInfo":{"status":"ok","timestamp":1633853649860,"user_tz":-540,"elapsed":7113,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}},"outputId":"be85e837-a8a1-4247-e138-3564ea8a44d8"},"source":["!pip install konlpy"],"id":"FHnOMBRDSNpy","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 30.5 MB/s \n","\u001b[?25hCollecting beautifulsoup4==4.6.0\n","  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 37.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1PvwJf_SQIp","executionInfo":{"status":"ok","timestamp":1633853675344,"user_tz":-540,"elapsed":16926,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}},"outputId":"bbb8e379-4ec5-416a-f199-48a874922353"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"-1PvwJf_SQIp","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"6409f8c3","executionInfo":{"status":"ok","timestamp":1633857468154,"user_tz":-540,"elapsed":251,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}}},"source":["import pandas as pd\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import re\n","import urllib.request\n","import time\n","from tqdm import tqdm\n","from konlpy.tag import Okt\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"id":"6409f8c3","execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QczGct6KODZs"},"source":["#네이버영화리뷰데이터로 모델생성"],"id":"QczGct6KODZs"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"id":"d2878c8f","executionInfo":{"status":"ok","timestamp":1633854194989,"user_tz":-540,"elapsed":604,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}},"outputId":"186ad6ff-4a89-4dad-cf50-66fe96ce3250"},"source":["#데이터 로드\n","train_data = pd.read_table('/content/drive/MyDrive/Colab Notebooks/ratings_train_all.txt')\n","train_data = train_data.sample(frac=0.1, random_state=42) ######일부만해봄\n","\n","#트레인테스트 스플릿\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, = train_test_split(train_data, test_size=0.25, random_state=42)\n","train_data = X_train\n","test_data = X_test\n","print('훈련용 리뷰 개수 :',len(train_data))\n","print('훈련용 리뷰 개수 :',len(test_data))\n","\n","# document 열에서 중복인 내용이 있다면 중복 제거\n","train_data.drop_duplicates(subset=['document'], inplace=True) \n","print()\n","print('중복제거 후 트레인셋 수 :',len(train_data))\n","\n","#결측치 제거\n","if train_data.isnull().values.any() ==True:\n","    train_data = train_data.dropna(how = 'any')\n","    print()\n","    print('결측치 제거 후 갯수 : ',len(train_data))\n","\n","# 트레인셋의 라벨 수\n","print()\n","print('트레인셋 라벨 수')\n","print(train_data.groupby('label').size().reset_index(name = 'count'))\n","\n","#한글 정규화 - 한글과 공백을 제외하고 모두 제거\n","train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","print()\n","print('한글정규화')\n","train_data[:5]"],"id":"d2878c8f","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련용 리뷰 개수 : 15000\n","훈련용 리뷰 개수 : 5000\n","\n","중복제거 후 트레인셋 수 : 14847\n","\n","결측치 제거 후 갯수 :  14846\n","\n","트레인셋 라벨 수\n","   label  count\n","0      0   7390\n","1      1   7456\n","\n","한글정규화\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>93453</th>\n","      <td>7550389</td>\n","      <td>보다가 중간에 나왔음</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>97061</th>\n","      <td>10129143</td>\n","      <td>안보신분들은 무조건 보시라고 추천하고 싶습니다 영화에 대해 별 사전지식 없이 봤는데...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>141690</th>\n","      <td>2845189</td>\n","      <td></td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>180651</th>\n","      <td>1135035</td>\n","      <td>여자배우가아깝다남자배우랑도 넘안어울려</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>42888</th>\n","      <td>7470122</td>\n","      <td>꿈과 현실 사이의 괴리에 좌절하는 청춘들의 모습을 담백하게 담아냈다 야구를 계속하는...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              id                                           document  label\n","93453    7550389                                        보다가 중간에 나왔음      0\n","97061   10129143  안보신분들은 무조건 보시라고 추천하고 싶습니다 영화에 대해 별 사전지식 없이 봤는데...      1\n","141690   2845189                                                         0\n","180651   1135035                               여자배우가아깝다남자배우랑도 넘안어울려      0\n","42888    7470122  꿈과 현실 사이의 괴리에 좌절하는 청춘들의 모습을 담백하게 담아냈다 야구를 계속하는...      1"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"090880b6","executionInfo":{"status":"ok","timestamp":1633854449943,"user_tz":-540,"elapsed":74207,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}},"outputId":"87bfedba-8c8e-4abc-ae88-ec49b293f22e"},"source":["print('다시 생긴 공백이 있다면 삭제')\n","train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n","train_data['document'].replace('', np.nan, inplace=True)\n","train_data = train_data.dropna(how = 'any')\n","print('학습데이터 길이 : ' + str(len(train_data)))\n","\n","#위와 마찬가지로 테스트셋도 전처리\n","test_data.drop_duplicates(subset = ['document'], inplace=True) # best_comment 열에서 중복인 내용이 있다면 중복 제거\n","test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n","test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n","test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","test_data = test_data.dropna(how='any') # Null 값 제거\n","print()\n","print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\n","\n","#불용어 지정 및 형태소 분리\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","\n","#형태소분리\n","okt = Okt()\n","\n","#트레인셋 형태소분리\n","X_train = []\n","for sentence in train_data['document']:\n","    temp_X = okt.morphs(sentence, stem=True) # 형태소분리\n","    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","    X_train.append(temp_X)\n","\n","#테스트셋 형태소분리\n","X_test = []\n","for sentence in test_data['document']:\n","  temp_X = okt.morphs(sentence, stem=True) # 형태소분리\n","  temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","  X_test.append(temp_X)"],"id":"090880b6","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["다시 생긴 공백이 있다면 삭제\n","학습데이터 길이 : 14746\n","\n","전처리 후 테스트용 샘플의 개수 : 4934\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bc01922d","executionInfo":{"status":"ok","timestamp":1633854529577,"user_tz":-540,"elapsed":7064,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}},"outputId":"c1672457-0a94-40ce-b5b4-3fd8f74454d5"},"source":["#정수인코딩\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(str(X_train))\n","\n","threshold = 3\n","total_cnt = len(tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n","\n","# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n","# 0번 패딩 토큰을 고려하여 + 1\n","vocab_size = total_cnt - rare_cnt + 1\n","print()\n","print('단어 집합의 크기 :',vocab_size)\n","\n","tokenizer = Tokenizer(vocab_size) \n","tokenizer.fit_on_texts(X_train)\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","\n","print()\n","print('정수인코딩 확인', X_train[:3])\n","\n","y_train = np.array(train_data['label'])\n","y_test = np.array(test_data['label'])\n","\n","print('빈도수 적은 단어들 삭제해서 빈 샘플들이 생겼음. 이를 다시 제거')\n","drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n","X_train = np.delete(X_train, drop_train, axis=0)\n","y_train = np.delete(y_train, drop_train, axis=0)\n","print(len(X_train))\n","print(len(y_train))\n","\n","print()\n","print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n","print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))"],"id":"bc01922d","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합(vocabulary)의 크기 : 1585\n","등장 빈도가 2번 이하인 희귀 단어의 수: 369\n","단어 집합에서 희귀 단어의 비율: 23.2807570977918\n","전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.07178597393710938\n","\n","단어 집합의 크기 : 1217\n","\n","정수인코딩 확인 [[2, 230, 26], [821, 950, 1050, 124, 242, 72, 80, 1, 449, 132, 175, 2, 375, 287, 10, 2, 25, 17, 13, 205, 16], [140, 50, 48, 194, 122, 540]]\n","14518\n","14518\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n"]}]},{"cell_type":"code","metadata":{"id":"b71bbd4a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633854694132,"user_tz":-540,"elapsed":260,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}},"outputId":"99ef6f1b-d0bb-4abe-dd13-e05b844a9c87"},"source":["#지정한 길이 이상길이 샘플 삭제\n","def below_threshold_len(max_len, nested_list):\n","  cnt = 0\n","  for s in nested_list:\n","    if(len(s) <= max_len):\n","        cnt = cnt + 1\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n","\n","max_len = 30  #길이지정\n","below_threshold_len(max_len, X_train)"],"id":"b71bbd4a","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["전체 샘플 중 길이가 30 이하인 샘플의 비율: 97.9473756715801\n"]}]},{"cell_type":"code","metadata":{"id":"e8cd939e","executionInfo":{"status":"ok","timestamp":1633854697162,"user_tz":-540,"elapsed":378,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}}},"source":["X_train = pad_sequences(X_train, maxlen = max_len)\n","X_test = pad_sequences(X_test, maxlen = max_len)"],"id":"e8cd939e","execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeQsnP8XPtwn"},"source":["lstm으로 네이버 영화 리뷰 감성 분류하기"],"id":"YeQsnP8XPtwn"},{"cell_type":"code","metadata":{"id":"38f6006a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633854769667,"user_tz":-540,"elapsed":52240,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}},"outputId":"3fea8d71-cf52-4113-e473-b75b159a3be9"},"source":["from tensorflow.keras.layers import Embedding, Dense, LSTM\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","#LSTM설정\n","model = Sequential()\n","model.add(Embedding(vocab_size, 100))\n","model.add(LSTM(128))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n","mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","\n","#학습시작\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n","\n","loaded_model = load_model('best_model.h5')\n","print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"],"id":"38f6006a","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","194/194 [==============================] - 12s 18ms/step - loss: 0.5023 - acc: 0.7537 - val_loss: 0.4394 - val_acc: 0.7999\n","\n","Epoch 00001: val_acc improved from -inf to 0.79993, saving model to best_model.h5\n","Epoch 2/15\n","194/194 [==============================] - 3s 14ms/step - loss: 0.3903 - acc: 0.8235 - val_loss: 0.4198 - val_acc: 0.8068\n","\n","Epoch 00002: val_acc improved from 0.79993 to 0.80682, saving model to best_model.h5\n","Epoch 3/15\n","194/194 [==============================] - 3s 14ms/step - loss: 0.3691 - acc: 0.8333 - val_loss: 0.4194 - val_acc: 0.8082\n","\n","Epoch 00003: val_acc improved from 0.80682 to 0.80820, saving model to best_model.h5\n","Epoch 4/15\n","194/194 [==============================] - 3s 14ms/step - loss: 0.3565 - acc: 0.8423 - val_loss: 0.4163 - val_acc: 0.8051\n","\n","Epoch 00004: val_acc did not improve from 0.80820\n","Epoch 5/15\n","194/194 [==============================] - 3s 14ms/step - loss: 0.3472 - acc: 0.8467 - val_loss: 0.4165 - val_acc: 0.8058\n","\n","Epoch 00005: val_acc did not improve from 0.80820\n","Epoch 6/15\n","194/194 [==============================] - 3s 14ms/step - loss: 0.3349 - acc: 0.8516 - val_loss: 0.4200 - val_acc: 0.7999\n","\n","Epoch 00006: val_acc did not improve from 0.80820\n","Epoch 7/15\n","194/194 [==============================] - 3s 15ms/step - loss: 0.3229 - acc: 0.8563 - val_loss: 0.4224 - val_acc: 0.8058\n","\n","Epoch 00007: val_acc did not improve from 0.80820\n","Epoch 8/15\n","194/194 [==============================] - 3s 14ms/step - loss: 0.3110 - acc: 0.8645 - val_loss: 0.4305 - val_acc: 0.8048\n","\n","Epoch 00008: val_acc did not improve from 0.80820\n","Epoch 00008: early stopping\n","155/155 [==============================] - 1s 5ms/step - loss: 0.4206 - acc: 0.8010\n","\n"," 테스트 정확도: 0.8010\n"]}]},{"cell_type":"markdown","metadata":{"id":"yjKwolMidOar"},"source":["#네이버영화리뷰데이터로 네이버웹툰댓글 감성점수 매기기"],"id":"yjKwolMidOar"},{"cell_type":"code","metadata":{"id":"730206e9","executionInfo":{"status":"ok","timestamp":1633856166154,"user_tz":-540,"elapsed":234,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}}},"source":["#파일 넣으면 알아서 해주는 모델 만들기\n","def sentiment_predict(new_sentence):\n","    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n","    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n","    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n","    pad_new = pad_sequences(encoded, maxlen = 1500) # 패딩\n","    score = float(loaded_model.predict(pad_new)) # 예측\n","    return score"],"id":"730206e9","execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"AKmIo_t9U4nT","executionInfo":{"status":"ok","timestamp":1633858020398,"user_tz":-540,"elapsed":1800,"user":{"displayName":"이영석","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05938684673482326731"}},"outputId":"c618a6ab-d0e3-474f-d198-1e5895aa555e"},"source":["#웹툰댓글파일 불러오기. 한글 43만여 댓글중 라벨링한 데이터 재외한 33만여 댓글들 분석\n","test_data = pd.read_csv('/content/drive/Shareddrives/데청캠/웹툰 세계화/데이터 모음집/FoDoDic_kw_진짜_최초라벨링.csv')\n","webtoon_comment = test_data[   ((test_data['pos'] ==0) & (test_data['neg'] ==0)) | \n","                               ((test_data['pos'] ==1) & (test_data['neg'] ==-1))       ]\n","\n","#점수매기기\n","pred_label=[]\n","for sentence in tqdm(webtoon_comment['best_comment']):\n","    try:\n","        pred_label.append(sentiment_predict(sentence)) \n","    except :\n","        pred_label.append(2)\n","\n","df = pd.DataFrame({'number' :webtoon_comment['number'],\n","                   'LabelfromNaverMovieReview' : pred_label,\n","                   'comment' : webtoon_comment['best_comment']} )\n","df"],"id":"AKmIo_t9U4nT","execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>best_comment</th>\n","      <th>pos</th>\n","      <th>neg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>BEST-신의탑을 않본사람은 있어도 한번본 사람은 없다-</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>BEST우리나라에서 유일하게 원피스,나루토,진격의거인이랑 맞먹는 웹툰이다ㅋㅋ</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>BESTSIU가 달려가면서 그림그리기를 자유자재로 구사하는것은 기본이고 선긋기같은 ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>BEST월요일을 위로 받는 방법</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>BEST탑에 꼭대기엔 작가님이 계시다</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>434555</th>\n","      <td>434555</td>\n","      <td>BEST바름유연 둘이 벌써 3주년이구나... 이제 슬슬 결혼하자</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>434563</th>\n","      <td>434563</td>\n","      <td>BEST루리웹, 클리앙, 보배드림, 축리웹 댓글테러단들 지금까지 뭐했어?맨날 열등감...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>434566</th>\n","      <td>434566</td>\n","      <td>BEST작가님 지켜드리지 못해 죄송합니다....다들 좀 조용히 했으면 좋겠는데 말이...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>434578</th>\n","      <td>434578</td>\n","      <td>BEST루리웹, 클리앙, 보배드림, 축리웹 댓글테러단들 지금까지 뭐했어?맨날 열등감...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>434581</th>\n","      <td>434581</td>\n","      <td>BEST작가님 지켜드리지 못해 죄송합니다....다들 좀 조용히 했으면 좋겠는데 말이...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>330832 rows × 4 columns</p>\n","</div>"],"text/plain":["        number                                       best_comment  pos  neg\n","0            0                    BEST-신의탑을 않본사람은 있어도 한번본 사람은 없다-    0    0\n","1            1         BEST우리나라에서 유일하게 원피스,나루토,진격의거인이랑 맞먹는 웹툰이다ㅋㅋ    0    0\n","2            2  BESTSIU가 달려가면서 그림그리기를 자유자재로 구사하는것은 기본이고 선긋기같은 ...    0    0\n","3            3                                  BEST월요일을 위로 받는 방법    0    0\n","4            4                               BEST탑에 꼭대기엔 작가님이 계시다    0    0\n","...        ...                                                ...  ...  ...\n","434555  434555                BEST바름유연 둘이 벌써 3주년이구나... 이제 슬슬 결혼하자    0    0\n","434563  434563  BEST루리웹, 클리앙, 보배드림, 축리웹 댓글테러단들 지금까지 뭐했어?맨날 열등감...    0    0\n","434566  434566  BEST작가님 지켜드리지 못해 죄송합니다....다들 좀 조용히 했으면 좋겠는데 말이...    0    0\n","434578  434578  BEST루리웹, 클리앙, 보배드림, 축리웹 댓글테러단들 지금까지 뭐했어?맨날 열등감...    0    0\n","434581  434581  BEST작가님 지켜드리지 못해 죄송합니다....다들 좀 조용히 했으면 좋겠는데 말이...    0    0\n","\n","[330832 rows x 4 columns]"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"193FinRaiffz"},"source":["#포도사전 - 위와 같은 방식으로, 직접라벨링한 댓글10만개를 토대로 나머지 댓글 분석"],"id":"193FinRaiffz"},{"cell_type":"code","metadata":{"id":"aGuH2728iesS"},"source":["#데이터 로드\n","train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/FoDoDic_kw_진짜_최초라벨링.csv')\n","\n","#필요한 칼럼만 부르고, 긍정이거나 부정으로 분류된 댓글만 뽑기\n","train_data = train_data[['number','best_comment','pos','neg']]\n","train_data = train_data[(train_data['pos']==1) | (train_data['neg']==-1)]\n","train_data\n","\n","#트레인 테스트셋 분리\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, = train_test_split(train_data, test_size=0.25, random_state=42)\n","train_data = X_train\n","test_data = X_test\n","\n","#중복 데이터 수\n","print(train_data['best_comment'].nunique(), train_data['pos'].nunique())\n","\n","# 중복 제거\n","train_data.drop_duplicates(subset=['best_comment'], inplace=True) \n","\n","#결측치있다면 삭제\n","if train_data.isnull().values.any() ==True:\n","    train_data = train_data.dropna(how = 'any')\n","    print('결측치 제거 후 갯수')\n","    len(train_data)\n","\n","#불용어지정\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","\n","#형태소분리\n","okt = Okt()\n","\n","#위의 과정을 테스트셋에도 \n","test_data['best_comment'].nunique(), test_data['pos'].nunique()\n","test_data.drop_duplicates(subset=['best_comment'], inplace=True) \n","if test_data.isnull().values.any() ==True:\n","    test_data = test_data.dropna(how = 'any')\n","    print('결측치 제거 후 갯수')\n","    len(test_data)\n","test_data\n","\n","X_train = []\n","for sentence in train_data['best_comment']:\n","    temp_X = okt.morphs(sentence, stem=True) # 형태소분리\n","    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","    X_train.append(temp_X)\n","\n","X_test = []\n","for sentence in test_data['best_comment']:\n","  temp_X = okt.morphs(sentence, stem=True) # 형태소분리\n","  temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","  X_test.append(temp_X)\n","\n","#정수인코딩\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(str(X_train))\n","\n","tokenizer.fit_on_texts(str(X_test))\n","\n","print(tokenizer.word_index)"],"id":"aGuH2728iesS","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"De0vBS-rie4x"},"source":["threshold = 3\n","total_cnt = len(tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","vocab_size = total_cnt - rare_cnt + 1\n","\n","tokenizer = Tokenizer(vocab_size) \n","tokenizer.fit_on_texts(X_train)\n","tokenizer.fit_on_texts(X_test)\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","\n","y_train = np.array(train_data['pos'])\n","y_test = np.array(test_data['pos'])\n","\n","# 빈도수 적은 단어들 삭제해서 다시 생긴 빈 샘플들 삭제\n","drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n","X_train = np.delete(X_train, drop_train, axis=0)\n","y_train = np.delete(y_train, drop_train, axis=0)\n","print(len(X_train))\n","print(len(y_train))\n","\n","#아래 길이 이상의 댓글 삭제\n","max_len = 60\n","below_threshold_len(max_len, X_train)\n","\n","X_train = pad_sequences(X_train, maxlen = max_len)\n","X_test = pad_sequences(X_test, maxlen = max_len)"],"id":"De0vBS-rie4x","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CsMlcECjBDL"},"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 100))\n","model.add(LSTM(128))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n","mc = ModelCheckpoint('best_model.h5', monitor='acc', mode='max', verbose=1, save_best_only=True)\n","\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"],"id":"0CsMlcECjBDL","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URJ72LirjU8E"},"source":["loaded_model = load_model('best_model.h5')\n","print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"],"id":"URJ72LirjU8E","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsMHsr9bjYd6"},"source":["#웹툰댓글파일 불러오기. 한글 43만여 댓글중 라벨링한 데이터 재외한 33만여 댓글들 분석\n","test_data = pd.read_csv('/content/drive/Shareddrives/데청캠/웹툰 세계화/데이터 모음집/FoDoDic_kw_진짜_최초라벨링.csv')\n","webtoon_comment = test_data[   ((test_data['pos'] ==0) & (test_data['neg'] ==0)) | \n","                               ((test_data['pos'] ==1) & (test_data['neg'] ==-1))       ]\n","\n","#점수매기기\n","pred_label=[]\n","for sentence in tqdm(webtoon_comment['best_comment']):\n","    try:\n","        pred_label.append(sentiment_predict(sentence)) \n","    except :\n","        pred_label.append(2)\n","\n","df = pd.DataFrame({'number' :webtoon_comment['number'],\n","                   'LabelfromNaverMovieReview' : pred_label,\n","                   'comment' : webtoon_comment['best_comment']} )\n","df"],"id":"jsMHsr9bjYd6","execution_count":null,"outputs":[]}]}